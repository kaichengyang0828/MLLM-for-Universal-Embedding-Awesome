# üìñ  Universal Multimodal Embedding Papers
A curated collection of cutting-edge research papers on universal multimodal embedding learning.

## üìö Paper List
| Title | Year | Paper | Github | 
|-------|------|------------------|------------------|
|E5-V: Universal embeddings with multimodal large language models| 2024 | [Paper](https://arxiv.org/abs/2407.12580) | [Github](https://github.com/kongds/E5-V)
|VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks|2024|[Paper](https://arxiv.org/abs/2410.05160)|[Github](https://github.com/TIGER-AI-Lab/VLM2Vec)|
ÔΩúMM-Embed: Universal Multimodal Retrieval with Multimodal LLMs|2024|[Paper](https://arxiv.org/abs/2411.02571)|[Huggingface](https://huggingface.co/nvidia/MM-Embed)
|Bridging Modalities: Improving Universal Multimodal Retrieval by Multimodal Large Language Models|2024|[Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Bridging_Modalities_Improving_Universal_Multimodal_Retrieval_by_Multimodal_Large_Language_CVPR_2025_paper.pdf)|[Huggingface](https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct)
|VladVA: Discriminative Fine-tuning of LVLMs|2024|[Paper](https://arxiv.org/abs/2412.04378)| Unreleased |
|LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant|2024|[Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_LamRA_Large_Multimodal_Model_as_Your_Advanced_Retrieval_Assistant_CVPR_2025_paper.pdf)|[Github](https://github.com/Code-kunkun/LamRA)
|Any Information Is Just Worth One Single Screenshot: Unifying Search With Visualized Information Retrieval|2025|[Paper](https://arxiv.org/pdf/2502.11431)|[Github](https://github.com/VectorSpaceLab/Vis-IR)
|LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning|2025|[Paper](https://arxiv.org/abs/2503.04812)|[Github](https://github.com/XMUDeepLIT/LLaVE)|
|CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning|2024|[Paper](https://arxiv.org/abs/2412.14783)|[Github](https://github.com/haoyu-bu/CAFe)|
|UniME: Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs|2025|[Paper](https://arxiv.org/abs/2504.17432)|[Github](https://github.com/deepglint/UniME)|
|Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining|2025|[Paper](https://arxiv.org/abs/2505.11293)|[Github](https://github.com/raghavlite/B3)|
|Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval|2025|[Paper](https://github.com/lst627/COCO-Facet)|Unreleased|
|mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data|2025|[Paper](https://arxiv.org/pdf/2502.08468)|[Github](https://github.com/haon-chen/mmE5)|
|Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval|2025|[Paper](https://arxiv.org/abs/2505.19650)|[Github](https://github.com/friedrichor/UNITE)|
|Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying|2025|[Paper](https://arxiv.org/abs/2506.02020)|[Github](https://github.com/QQ-MM/QQMM-embed)|
|Incorporating Dense Knowledge Alignment into Unified Multimodal Representation Models|2025|[Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Cui_Incorporating_Dense_Knowledge_Alignment_into_Unified_Multimodal_Representation_Models_CVPR_2025_paper.pdf)|Unreleased|
|Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment|2025|[Paper](https://arxiv.org/abs/2506.06970)|[Github](https://github.com/zjunlp/MALP)|Unreleased|
|MM-R5: MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval|2025|[Paper](https://arxiv.org/abs/2506.12364)|[Github](https://github.com/i2vec/MM-R5)|
|jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval|2025|[Paper](https://arxiv.org/abs/2506.18902)|[Huggingface](https://huggingface.co/jinaai/jina-embeddings-v4)|
|PUMA: Layer-Pruned Language Model for Efficient Unified Multimodal Retrieval with Modality-Adaptive Learning|2025|[Paper](https://arxiv.org/pdf/2507.08064)|Unreleased|
|UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings|2025|[Paper](https://arxiv.org/pdf/2505.11815)|[Github](https://github.com/hobbitqia/unimoco)|
|U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs|2025|[Paper](https://arxiv.org/pdf/2507.14902)|[Huggingface](https://huggingface.co/TencentBAC/U-MARVEL-Qwen2VL-7B-Instruct)|
|Context-Adaptive Multi-Prompt Embedding with Large Language Models for Vision-Language Alignment|2025|[Paper](https://arxiv.org/pdf/2508.02762)|Unreleased|
|From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model|2025|[Paper](https://arxiv.org/pdf/2508.00955)|[Github](https://github.com/yeongjoonJu/Gen2Embed)|
|WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM|2025|[Paper](https://arxiv.org/pdf/2509.21990)|Unreleased|
|Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval|2025|[Paper](https://arxiv.org/pdf/2510.02745)|[Github](https://github.com/TencentBAC/Retrv-R1)|Unreleased|
|VIRTUE: Visual-Interactive Text-Image Universal Embedder|2025|[Paper](https://arxiv.org/pdf/2510.00523)|Unreleased|
|Vlm2vec-v2: Advancing multimodal embedding for videos, images, and visual documents|2025|[Paper](https://arxiv.org/abs/2507.04590)|[Github](https://github.com/TIGER-AI-Lab/VLM2Vec)|
|CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning|2025|[Paper](https://arxiv.org/pdf/2510.08003)|Unreleased|
|SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model|2025|[Paper](https://arxiv.org/pdf/2510.12709)|Unreleased|
|Multi-Modal Multi-Task Unified Embedding Model (M3T-UEM): A Task-Adaptive Representation Learning Framework|2025|[Paper](https://openaccess.thecvf.com/content/ICCV2025/papers/Sharma_Multi-Modal_Multi-Task_Unified_Embedding_Model_M3T-UEM_A_Task-Adaptive_Representation_Learning_ICCV_2025_paper.pdf)|Unreleased|
|Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation|2025|[Paper](https://arxiv.org/pdf/2510.17354)|[Github](https://github.com/SnowNation101/Nyx)|
|Think Then Embed: Generative Context Improves Multimodal Embedding|2025|[Paper](https://arxiv.org/pdf/2510.05014)|Unreleased|
|MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction|2025|[Paper](https://arxiv.org/pdf/2509.18095)|Unreleased|
|UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning|2025|[Paper]((https://arxiv.org/pdf/2510.13515))|[Github](https://github.com/GaryGuTC/UniME-v2)|
|MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval|2025|[Paper](https://arxiv.org/abs/2510.15543)|Unreleased|
|RzenEmbed: Towards Comprehensive Multimodal Retrieval|2025|[Paper](https://arxiv.org/pdf/2510.27350)|[Huggingface](https://huggingface.co/qihoo360/RzenEmbed)|
|Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation|2025|[Paper](https://arxiv.org/pdf/2511.02358)|Unreleased|
|Scaling Language-Centric Omnimodal Representation Learning|2025|[Paper](https://arxiv.org/pdf/2510.11693)|[Github](https://github.com/LCO-Embedding/LCO-Embedding)|
|Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding|2025|[Paper](https://arxiv.org/pdf/2511.08480)|Unreleased|
|MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding|2025|[Paper](https://arxiv.org/pdf/2508.11999)|Unreleased|
|MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding|2025|[Paper](https://arxiv.org/pdf/2511.12449)|Unreleased|
|ReMatch: Boosting Representation through Matching for Multimodal Retrieval|2025|[Paper](https://arxiv.org/pdf/2511.19431)|[Github](https://github.com/FireRedTeam/ReMatch)|
|FANoise: Singular Value-Adaptive Noise Modulation for Robust Multimodal Representation Learning|2025|[Paper](https://arxiv.org/pdf/2511.20997)|Unreleased|
|Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval|2025|[Paper](https://arxiv.org/pdf/2511.16150)|[Github](https://github.com/MCG-NJU/RGE)|
|Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization|2025|[Paper](https://arxiv.org/pdf/2511.01588)|[Github](https://github.com/Xu3XiWang/PDF-VLM2Vec)|
|e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings|2026|[Paper](https://arxiv.org/pdf/2601.03666)|[Huggingface](https://huggingface.co/Haon-Chen/e5-omni-7B)|
|Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking|2026|[Paper](https://github.com/QwenLM/Qwen3-VL-Embedding/blob/main/assets/qwen3vlembedding_technical_report.pdf)|[Huggingface](https://huggingface.co/Qwen/Qwen3-VL-Embedding-8B)|



## üìù Contributing
Continuously updating the list. Feel free to contribute by:
- Adding new relevant papers
- Reporting issues or broken links

## üéôÔ∏è Who We Are?

We are the **UniME** team, passionate about learning more robust representations through MLLM to empower various downstream tasks.
If you are interest in universal multimodal embedding learning or would like to collaborate with us, feel free to reach out: kaichengyang0828@gmail.com

